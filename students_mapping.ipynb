{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851dc07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QUESTION ‚Üí CONCEPT TAGGING FOR JUPYTER NOTEBOOK (CLEAN + ROBUST + FIXED)\n",
    "# ============================================================================\n",
    "# NOTE:\n",
    "# - Structured Outputs JSON schema MUST have a ROOT object (not a root array).\n",
    "#   So we return: { \"concepts\": [ ... ] }\n",
    "# - Do NOT commit your API key to git. If your key was exposed, rotate it.\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 1: Install dependencies (run once)\n",
    "# ----------------------------------------------------------------------------\n",
    "# !pip install -U openai numpy\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 2: Imports + API key\n",
    "# ----------------------------------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "# Put your key here locally (do NOT commit):\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 3: Configuration\n",
    "# ----------------------------------------------------------------------------\n",
    "TAGGER_MODEL = \"gpt-4o-mini\"             # tagging model\n",
    "EMBED_MODEL  = \"text-embedding-3-small\"  # for prefiltering concepts (optional)\n",
    "\n",
    "# Choose how many concepts per question:\n",
    "#   1 => ONE primary concept\n",
    "#   2/3 => top 2‚Äì3 concepts\n",
    "NUM_CONCEPTS = 3\n",
    "MIN_CONFIDENCE = 0.70\n",
    "\n",
    "# If your KG has many concepts, don't pass all of them to the LLM.\n",
    "# Use embeddings to prefilter to top candidates.\n",
    "USE_EMBEDDING_PREFILTER = True\n",
    "CANDIDATE_POOL_SIZE = 60       # how many candidate concepts to show the LLM\n",
    "EMBED_BATCH_SIZE = 256         # batch size for embedding API calls\n",
    "EMBED_CACHE_PATH = \"concept_embeddings.npz\"  # local cache\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 4: Prompt (aligned with ROOT OBJECT schema)\n",
    "# ----------------------------------------------------------------------------\n",
    "PROMPT = \"\"\"\n",
    "You are an expert database teaching assistant mapping SQL questions to the MOST relevant concepts from the knowledge graph.\n",
    "\n",
    "You will be given:\n",
    "1) A SQL problem/question statement\n",
    "2) A list of candidate concept names that already exist in the knowledge graph\n",
    "\n",
    "Your job:\n",
    "- Choose the MOST relevant concepts from the candidate list\n",
    "- These should be the PRIMARY SQL concepts/skills needed to solve this question\n",
    "- Copy each concept name EXACTLY as it appears (preserve case, spaces, underscores)\n",
    "- Do NOT invent new concepts - only use concepts from the provided list\n",
    "- Prefer HIGH-LEVEL concepts (JOIN, SUBQUERY, GROUP_BY, HAVING, etc.) over specific values\n",
    "\n",
    "Return ONLY a valid JSON OBJECT with this exact structure:\n",
    "{\n",
    "  \"concepts\": [\n",
    "    {\n",
    "      \"concept_name\": \"EXACT_CONCEPT_NAME_FROM_LIST\",\n",
    "      \"confidence\": 0.90,\n",
    "      \"explanation\": \"Brief reason why this concept is needed\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Important:\n",
    "- Order by importance (most important first)\n",
    "- If NUM_CONCEPTS >= 2, return 2‚Äì3 concepts if possible\n",
    "- Return ONLY the JSON object, no other text\n",
    "\"\"\".strip()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 5: KG concept extraction\n",
    "# ----------------------------------------------------------------------------\n",
    "def extract_concepts_from_kg(kg_jsonl_path: str) -> List[str]:\n",
    "    \"\"\"Extract all unique concept names from KG jsonl with edges containing A/B dicts.\"\"\"\n",
    "    concepts = set()\n",
    "    with open(kg_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            for key in (\"A\", \"B\"):\n",
    "                if key in obj and isinstance(obj[key], dict):\n",
    "                    name = obj[key].get(\"name\", \"\")\n",
    "                    if isinstance(name, str) and len(name.strip()) > 1:\n",
    "                        concepts.add(name.strip())\n",
    "\n",
    "    concept_list = sorted(concepts)\n",
    "    print(f\"‚úì Extracted {len(concept_list)} unique concepts from KG\")\n",
    "    return concept_list\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 6: Embedding helpers (optional prefilter)\n",
    "# ----------------------------------------------------------------------------\n",
    "def _l2_normalize(mat: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    denom = np.linalg.norm(mat, axis=1, keepdims=True) + eps\n",
    "    return mat / denom\n",
    "\n",
    "def embed_texts(texts: List[str], model: str = EMBED_MODEL, batch_size: int = EMBED_BATCH_SIZE) -> np.ndarray:\n",
    "    \"\"\"Embed a list of texts using OpenAI embeddings endpoint.\"\"\"\n",
    "    all_vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        resp = client.embeddings.create(model=model, input=batch)\n",
    "        vecs = [d.embedding for d in resp.data]\n",
    "        all_vecs.append(np.array(vecs, dtype=np.float32))\n",
    "    return np.vstack(all_vecs)\n",
    "\n",
    "def load_or_build_concept_embeddings(concepts: List[str], cache_path: str = EMBED_CACHE_PATH) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build (or load cached) normalized embeddings for concept strings.\n",
    "    Cache is invalidated automatically if the concept list changes.\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            data = np.load(cache_path, allow_pickle=True)\n",
    "            cached_concepts = data[\"concepts\"].tolist()\n",
    "            cached_embeds = data[\"embeddings\"]\n",
    "            if cached_concepts == concepts:\n",
    "                print(f\"‚úì Loaded cached concept embeddings: {cache_path}\")\n",
    "                return cached_embeds\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Concept list changed ‚Äî rebuilding embeddings cache.\")\n",
    "        except Exception:\n",
    "            print(\"‚ö†Ô∏è  Failed to load cache ‚Äî rebuilding embeddings cache.\")\n",
    "\n",
    "    print(f\"üîé Building embeddings for {len(concepts)} concepts (this can take a bit)...\")\n",
    "    embeds = embed_texts(concepts, model=EMBED_MODEL, batch_size=EMBED_BATCH_SIZE)\n",
    "    embeds = _l2_normalize(embeds)\n",
    "\n",
    "    np.savez_compressed(cache_path, concepts=np.array(concepts, dtype=object), embeddings=embeds)\n",
    "    print(f\"‚úì Saved embeddings cache to: {cache_path}\")\n",
    "    return embeds\n",
    "\n",
    "def select_candidate_concepts(\n",
    "    question_text: str,\n",
    "    concepts: List[str],\n",
    "    concept_embeds: Optional[np.ndarray],\n",
    "    k: int = CANDIDATE_POOL_SIZE\n",
    ") -> List[str]:\n",
    "    \"\"\"Shortlist candidate concepts for the LLM.\"\"\"\n",
    "    if (not USE_EMBEDDING_PREFILTER) or (concept_embeds is None) or (len(concepts) <= k):\n",
    "        return concepts if len(concepts) <= k else concepts[:k]\n",
    "\n",
    "    q_vec = embed_texts([question_text], model=EMBED_MODEL, batch_size=1)\n",
    "    q_vec = _l2_normalize(q_vec)[0]\n",
    "\n",
    "    scores = concept_embeds @ q_vec  # cosine similarity (both normalized)\n",
    "    top_idx = np.argsort(-scores)[:k]\n",
    "    return [concepts[i] for i in top_idx]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 7: LLM tagging (Structured Outputs JSON schema) ‚Äî FIXED ROOT OBJECT\n",
    "# ----------------------------------------------------------------------------\n",
    "def tag_question(\n",
    "    question_text: str,\n",
    "    candidate_concepts: List[str],\n",
    "    num_concepts: int = NUM_CONCEPTS,\n",
    "    min_confidence: float = MIN_CONFIDENCE\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Tag a question with concepts using Responses API + JSON schema.\n",
    "    Returns a list of tag objects (length 0..num_concepts).\n",
    "    \"\"\"\n",
    "    safe_concepts = [c for c in candidate_concepts if isinstance(c, str) and c.strip()]\n",
    "    if not safe_concepts:\n",
    "        return []\n",
    "\n",
    "    # Root MUST be an object, so we use { \"concepts\": [ ... ] }\n",
    "    min_items = 1 if num_concepts <= 1 else 2\n",
    "    schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"additionalProperties\": False,\n",
    "        \"properties\": {\n",
    "            \"concepts\": {\n",
    "                \"type\": \"array\",\n",
    "                \"minItems\": min_items,\n",
    "                \"maxItems\": max(1, num_concepts),\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"additionalProperties\": False,\n",
    "                    \"properties\": {\n",
    "                        \"concept_name\": {\"type\": \"string\"},\n",
    "                        \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n",
    "                        \"explanation\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"required\": [\"concept_name\", \"confidence\", \"explanation\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"concepts\"]\n",
    "    }\n",
    "\n",
    "    full_prompt = (\n",
    "        f\"{PROMPT}\\n\\n\"\n",
    "        f\"SQL Question/Problem:\\n{question_text}\\n\\n\"\n",
    "        \"Available Concepts in Knowledge Graph:\\n\" +\n",
    "        \"\\n\".join(f\"- {c}\" for c in safe_concepts)\n",
    "    )\n",
    "\n",
    "    resp = client.responses.create(\n",
    "        model=TAGGER_MODEL,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise educational concept tagging assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt},\n",
    "        ],\n",
    "        text={\n",
    "            \"format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"name\": \"concept_tagging\",\n",
    "                \"schema\": schema,\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    raw = resp.output_text.strip()\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "    except Exception:\n",
    "        print(\"‚ùå Failed to parse model output as JSON.\")\n",
    "        return []\n",
    "\n",
    "    tags = parsed.get(\"concepts\", [])\n",
    "    if not isinstance(tags, list):\n",
    "        return []\n",
    "\n",
    "    # Filter by confidence\n",
    "    tags = [t for t in tags if float(t.get(\"confidence\", 0)) >= min_confidence]\n",
    "\n",
    "    # Deduplicate by concept_name, preserve order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for t in tags:\n",
    "        name = t.get(\"concept_name\")\n",
    "        if isinstance(name, str) and name.strip() and name not in seen:\n",
    "            seen.add(name)\n",
    "            out.append(t)\n",
    "\n",
    "    return out[:max(1, num_concepts)]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 8: Batch processing\n",
    "# ----------------------------------------------------------------------------\n",
    "def process_questions_batch(\n",
    "    questions_data: List[Dict[str, Any]],\n",
    "    kg_jsonl_path: str,\n",
    "    output_path: str,\n",
    "    num_concepts: int = NUM_CONCEPTS,\n",
    "    min_confidence: float = MIN_CONFIDENCE\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BATCH QUESTION ‚Üí CONCEPT TAGGING\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    print(f\"üìö Extracting concepts from: {kg_jsonl_path}\")\n",
    "    concepts = extract_concepts_from_kg(kg_jsonl_path)\n",
    "\n",
    "    concept_embeds = None\n",
    "    if USE_EMBEDDING_PREFILTER and len(concepts) > CANDIDATE_POOL_SIZE:\n",
    "        concept_embeds = load_or_build_concept_embeddings(concepts, cache_path=EMBED_CACHE_PATH)\n",
    "\n",
    "    print(f\"\\nFound {len(concepts)} total KG concepts\")\n",
    "    if USE_EMBEDDING_PREFILTER and concept_embeds is not None:\n",
    "        print(f\"Using embeddings prefilter ‚Üí LLM sees top {CANDIDATE_POOL_SIZE} candidates per question\")\n",
    "    else:\n",
    "        print(\"No embeddings prefilter ‚Üí LLM sees full (or truncated) concept list\")\n",
    "\n",
    "    tagged_questions = []\n",
    "\n",
    "    for i, q in enumerate(questions_data, 1):\n",
    "        qid = q.get(\"question_id\", f\"Q{i}\")\n",
    "        qtext = (q.get(\"question_text\", \"\") or \"\").strip()\n",
    "\n",
    "        print(f\"\\n[{i}/{len(questions_data)}] Processing {qid}...\")\n",
    "        print(f\"   Question: {qtext[:90].replace('\\\\n',' ')}...\")\n",
    "\n",
    "        if not qtext:\n",
    "            tags = []\n",
    "            print(\"   ‚ö†Ô∏è  No question text ‚Üí skipped tagging\")\n",
    "        else:\n",
    "            candidates = select_candidate_concepts(\n",
    "                question_text=qtext,\n",
    "                concepts=concepts,\n",
    "                concept_embeds=concept_embeds,\n",
    "                k=CANDIDATE_POOL_SIZE\n",
    "            )\n",
    "            tags = tag_question(\n",
    "                question_text=qtext,\n",
    "                candidate_concepts=candidates,\n",
    "                num_concepts=num_concepts,\n",
    "                min_confidence=min_confidence\n",
    "            )\n",
    "\n",
    "            if tags:\n",
    "                print(\"   ‚úÖ Tagged with:\")\n",
    "                for t in tags:\n",
    "                    print(f\"      - {t['concept_name']} (conf: {t['confidence']:.2f})\")\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è  No concepts assigned (below confidence threshold)\")\n",
    "\n",
    "        record = {\n",
    "            \"question_id\": qid,\n",
    "            \"question_text\": qtext,\n",
    "        }\n",
    "\n",
    "        if num_concepts <= 1:\n",
    "            record[\"primary_concept\"] = tags[0] if tags else None\n",
    "        else:\n",
    "            record[\"concepts\"] = tags\n",
    "\n",
    "        tagged_questions.append(record)\n",
    "\n",
    "    output_data = {\n",
    "        \"total_questions\": len(tagged_questions),\n",
    "        \"num_concepts_per_question\": num_concepts,\n",
    "        \"min_confidence\": min_confidence,\n",
    "        \"questions\": tagged_questions\n",
    "    }\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    tagged_count = sum(\n",
    "        1 for r in tagged_questions\n",
    "        if (r.get(\"primary_concept\") is not None) or (len(r.get(\"concepts\", [])) > 0)\n",
    "    )\n",
    "    print(f\"Total questions processed: {len(tagged_questions)}\")\n",
    "    print(f\"Successfully tagged: {tagged_count}/{len(tagged_questions)}\")\n",
    "    print(f\"üíæ Results saved to: {output_path}\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    return output_data\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 9: Define your questions\n",
    "# ----------------------------------------------------------------------------\n",
    "questions = [\n",
    "    {\n",
    "        \"question_id\": \"Q1\",\n",
    "        \"question_text\": \"\"\".\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question_id\": \"Q2\",\n",
    "        \"question_text\": \"\"\"\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question_id\": \"Q3\",\n",
    "        \"question_text\": \"\"\"\"\"\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 10: Run tagging\n",
    "# ----------------------------------------------------------------------------\n",
    "result = process_questions_batch(\n",
    "    questions_data=questions,\n",
    "    kg_jsonl_path=\"MAIN_sql_qwen14b.jsonl\",\n",
    "    output_path=\"tagged_questions.json\",\n",
    "    num_concepts=NUM_CONCEPTS,\n",
    "    min_confidence=MIN_CONFIDENCE\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 11: View results\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\nüìä DETAILED RESULTS:\\n\")\n",
    "for q in result[\"questions\"]:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Question: {q['question_id']}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if NUM_CONCEPTS <= 1:\n",
    "        pc = q.get(\"primary_concept\")\n",
    "        if pc:\n",
    "            print(f\"‚úÖ Concept: {pc['concept_name']}\")\n",
    "            print(f\"   Confidence: {pc['confidence']:.2f}\")\n",
    "            print(f\"   Reason: {pc['explanation']}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No concept assigned\")\n",
    "    else:\n",
    "        concepts = q.get(\"concepts\", [])\n",
    "        if concepts:\n",
    "            print(f\"‚úÖ Tagged with {len(concepts)} concept(s):\")\n",
    "            for i, c in enumerate(concepts, 1):\n",
    "                print(f\"  {i}. {c['concept_name']} (confidence: {c['confidence']:.2f})\")\n",
    "                print(f\"     ‚Üí {c['explanation']}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No concepts assigned\")\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
